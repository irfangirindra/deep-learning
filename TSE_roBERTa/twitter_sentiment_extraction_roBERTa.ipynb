{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/tf-roberta/config-roberta-base.json\n",
      "/kaggle/input/tf-roberta/vocab-roberta-base.json\n",
      "/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n",
      "/kaggle/input/tf-roberta/merges-roberta-base.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 3 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Test Data To Array For roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyperparameter on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout = 0.2     # originally 0.1\n",
    "n_split = 5            # originally 5\n",
    "lr = 1e-5              # originally 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(Dropout)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(Dropout)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "687/687 [==============================] - 107s 156ms/step - loss: 3.1829 - activation_loss: 1.5898 - activation_1_loss: 1.5931 - val_loss: 2.6499 - val_activation_loss: 1.3216 - val_activation_1_loss: 1.3283\n",
      "Epoch 2/2\n",
      "687/687 [==============================] - 100s 146ms/step - loss: 2.6444 - activation_loss: 1.3320 - activation_1_loss: 1.3123 - val_loss: 2.5419 - val_activation_loss: 1.2841 - val_activation_1_loss: 1.2579\n",
      "Epoch 3/3\n",
      "687/687 [==============================] - 100s 145ms/step - loss: 2.5450 - activation_loss: 1.2823 - activation_1_loss: 1.2627 - val_loss: 2.5124 - val_activation_loss: 1.2670 - val_activation_1_loss: 1.2454\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 13s 74ms/step\n",
      "Predicting all Train for Outlier analysis...\n",
      "859/859 [==============================] - 63s 74ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 8s 72ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.7114669051421523\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "688/688 [==============================] - 105s 152ms/step - loss: 3.1830 - activation_loss: 1.5849 - activation_1_loss: 1.5981 - val_loss: 2.6252 - val_activation_loss: 1.3441 - val_activation_1_loss: 1.2811\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 102s 149ms/step - loss: 2.6504 - activation_loss: 1.3395 - activation_1_loss: 1.3109 - val_loss: 2.4991 - val_activation_loss: 1.2816 - val_activation_1_loss: 1.2175\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 107s 156ms/step - loss: 2.5729 - activation_loss: 1.3039 - activation_1_loss: 1.2690 - val_loss: 2.4907 - val_activation_loss: 1.2708 - val_activation_1_loss: 1.2199\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 12s 72ms/step\n",
      "Predicting all Train for Outlier analysis...\n",
      "688/688 [==============================] - 101s 147ms/step - loss: 2.6102 - activation_loss: 1.3223 - activation_1_loss: 1.2879 - val_loss: 2.5349 - val_activation_loss: 1.2798 - val_activation_1_loss: 1.2551\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 104s 151ms/step - loss: 2.5340 - activation_loss: 1.2865 - activation_1_loss: 1.2475 - val_loss: 2.5155 - val_activation_loss: 1.2663 - val_activation_1_loss: 1.2492\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 12s 72ms/step\n",
      "Predicting all Train for Outlier analysis...\n",
      "859/859 [==============================] - 63s 74ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 8s 74ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.7059539409677048\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "688/688 [==============================] - 106s 154ms/step - loss: 3.1648 - activation_loss: 1.5836 - activation_1_loss: 1.5811 - val_loss: 2.6563 - val_activation_loss: 1.3619 - val_activation_1_loss: 1.2944\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 103s 149ms/step - loss: 2.6286 - activation_loss: 1.3291 - activation_1_loss: 1.2995 - val_loss: 2.5574 - val_activation_loss: 1.3033 - val_activation_1_loss: 1.2541\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 110s 161ms/step - loss: 2.5824 - activation_loss: 1.3102 - activation_1_loss: 1.2722 - val_loss: 2.5352 - val_activation_loss: 1.2942 - val_activation_1_loss: 1.2410\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 13s 75ms/step\n",
      "Predicting all Train for Outlier analysis...\n",
      "726/859 [========================>.....] - ETA: 9s"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_split,shuffle=True,random_state=SEED)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "        \n",
    "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
    "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
    "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting all Train for Outlier analysis...')\n",
    "    preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n",
    "    preds_start_train += preds_train[0]/skf.n_splits\n",
    "    preds_end_train += preds_train[1]/skf.n_splits\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> OVERALL 5Fold CV Jaccard = 0.7065620070593142\n",
      "[0.7114669051421523, 0.7124690816023932, 0.7059539409677048, 0.6982741388805002, 0.7046459687038202]\n"
     ]
    }
   ],
   "source": [
    "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\n",
    "print(jac) # Jaccard CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>12005b65fc</td>\n",
       "      <td>Waiting for my turn on wii fit gym closed</td>\n",
       "      <td>neutral</td>\n",
       "      <td>waiting for my turn on wii fit gym closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>bcf13877f7</td>\n",
       "      <td>Good morning everyone</td>\n",
       "      <td>positive</td>\n",
       "      <td>good morning everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>575e4a89fe</td>\n",
       "      <td>tts ridiculously sweet of you</td>\n",
       "      <td>positive</td>\n",
       "      <td>ridiculously sweet of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>a0b1828b67</td>\n",
       "      <td>'Brides a la mode' pow wow first thing this mo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>472c3e2c41</td>\n",
       "      <td>Getting somewhere with my first 'real' KiokuDB...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>getting somewhere with my first 'real' kiokud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>ce71d002ec</td>\n",
       "      <td>Mommas day is may 10th! Don`t forget to do som...</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>8db4aaef4a</td>\n",
       "      <td>watching the notebook</td>\n",
       "      <td>neutral</td>\n",
       "      <td>watching the notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>895de1648c</td>\n",
       "      <td>really tired. and have to work the whole day t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>really tired.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>78d89e7c64</td>\n",
       "      <td>Yeah prbly pickin up songs for SingStar. Have...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>yeah prbly pickin up songs for singstar. have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>756d255e40</td>\n",
       "      <td>is at home with a pukey boy! Poor little baby</td>\n",
       "      <td>negative</td>\n",
       "      <td>poor little baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "1191  12005b65fc          Waiting for my turn on wii fit gym closed   neutral   \n",
       "661   bcf13877f7                              Good morning everyone  positive   \n",
       "1968  575e4a89fe                      tts ridiculously sweet of you  positive   \n",
       "826   a0b1828b67  'Brides a la mode' pow wow first thing this mo...  positive   \n",
       "992   472c3e2c41  Getting somewhere with my first 'real' KiokuDB...   neutral   \n",
       "1922  ce71d002ec  Mommas day is may 10th! Don`t forget to do som...  positive   \n",
       "1043  8db4aaef4a                              watching the notebook   neutral   \n",
       "2111  895de1648c  really tired. and have to work the whole day t...  negative   \n",
       "1695  78d89e7c64   Yeah prbly pickin up songs for SingStar. Have...   neutral   \n",
       "2220  756d255e40      is at home with a pukey boy! Poor little baby  negative   \n",
       "\n",
       "                                          selected_text  \n",
       "1191          waiting for my turn on wii fit gym closed  \n",
       "661                               good morning everyone  \n",
       "1968                          ridiculously sweet of you  \n",
       "826                                              lovely  \n",
       "992    getting somewhere with my first 'real' kiokud...  \n",
       "1922                                               nice  \n",
       "1043                              watching the notebook  \n",
       "2111                                      really tired.  \n",
       "1695   yeah prbly pickin up songs for singstar. have...  \n",
       "2220                                   poor little baby  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "test.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
